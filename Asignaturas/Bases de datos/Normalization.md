Normalization is a process of organizing the columns (attributes) and tables (relations) in a relational database to reduce data redundancy and improve data integrity. It involves breaking down a table into smaller tables and defining relationships between them. The goal is to isolate data so that additions, deletions, and modifications of attributes can be made in just one table, thus avoiding inconsistencies.

**Concept:** It's like tidying up your data! You want to store information in a way that avoids repeating yourself and makes sure updates are consistent everywhere. You achieve this by following a set of rules called "normal forms."

**Why Normalize?** ü§î

* **Minimizing Redundancy:** Avoids storing the same information multiple times, saving space and reducing the risk of inconsistencies. üíæ‚ùå
* **Improving Data Integrity:** Ensures that data is accurate and consistent across the database. ‚úÖüîí
* **Easier Maintenance:** Makes it simpler to update, delete, or insert data without causing problems in other parts of the database. üõ†Ô∏è effortless

**Normal Forms:**

Normalization is typically done in stages, following what are called normal forms. The most common are:

* **First Normal Form (1NF):** Eliminates repeating groups of attributes and ensures that each attribute contains atomic (indivisible) values. 1Ô∏è‚É£
* **Second Normal Form (2NF):** Meets 1NF and ensures that all non-key attributes are fully functionally dependent on the primary key. (Applies to tables with composite primary keys). 2Ô∏è‚É£
* **Third Normal Form (3NF):** Meets 2NF and eliminates transitive dependencies (where a non-key attribute is dependent on another non-key attribute). 3Ô∏è‚É£
* **Boyce-Codd Normal Form (BCNF):** A stricter version of 3NF. (Every determinant is a candidate key). üîê

**Example of Normalization (Simplistic):**

Let's say we start with a single table `üì¶ OrderDetails` that is not normalized:

| OrderID | ProductID | ProductName | ProductPrice | CustomerName | CustomerAddress |
|---|---|---|---|---|---|
| 101 | A1 | Laptop | 1000 | Alice | 123 Main St |
| 101 | B2 | Mouse | 25 | Alice | 123 Main St |
| 102 | A1 | Laptop | 1000 | Bob | 456 Oak Ave |

**Problems:**

* **Redundancy:** Customer information (Name, Address) is repeated for each product in an order. Product details (ProductName, ProductPrice) are repeated for each order containing that product. üëéüíæ
* **Update Anomalies:** If Alice changes her address, you have to update multiple rows. If you delete the last product for an order, you lose the customer information for that order. üõ†Ô∏èüîÑ‚ùì

**After Normalization (e.g., towards 3NF):**

We can break this down into three tables:

`üõçÔ∏è Orders` table:

| OrderID | CustomerName | CustomerAddress |
|---|---|---|
| 101 | Alice | 123 Main St |
| 102 | Bob | 456 Oak Ave |

`üçé Products` table:

| ProductID | ProductName | ProductPrice |
|---|---|---|
| A1 | Laptop | 1000 |
| B2 | Mouse | 25 |

`üõí OrderItems` table (linking Orders and Products):

| OrderID | ProductID |
|---|---|
| 101 | A1 |
| 101 | B2 |
| 102 | A1 |

**Benefits:**

* Customer information is stored only once in `üõçÔ∏è Orders`.
* Product information is stored only once in `üçé Products`.
* Relationships are managed in `üõí OrderItems`.
* Updates are easier and inconsistencies are avoided. ‚úÖ‚ú®

Functional dependencies are the rules that guide the normalization process, helping us identify how to decompose tables correctly to achieve the desired normal form.

# Understanding Insertion Anomalies in Databases

In the realm of database design and normalization, **anomalies** are undesirable conditions that can lead to data inconsistencies, redundancies, and difficulties in managing data. There are three primary types of anomalies: insertion, deletion, and update anomalies. This document focuses on explaining the **Insertion Anomaly**.

## What is an Insertion Anomaly?

An **insertion anomaly** occurs when you are unable to insert certain information into a database table without the presence of other, often unrelated, data. Essentially, the structure of the table forces you to enter data for attributes that may not be available or applicable at the time of insertion, or it prevents you from adding a new record if a related piece of information is missing.

This can also manifest as a situation where adding new, valid data to a table creates inconsistencies or requires data to be entered redundantly.

## How Do Insertion Anomalies Occur?

Insertion anomalies are typically a symptom of a poorly designed database schema, specifically one that is not sufficiently normalized. Key causes include:

1.  **Data Redundancy:** When the same information is repeated in multiple rows, inserting a new, distinct piece of information might require duplicating existing data unnecessarily.
2.  **Improper Grouping of Attributes:** Storing multiple, distinct "facts" or entity types within a single table can lead to situations where you can't record one fact without also having information for the other.
3.  **Primary Key Constraints:** If unrelated attributes are part of a composite primary key, or if a foreign key constraint forces the existence of a related record that isn't logically necessary for the new record's existence.
4.  **NOT NULL Constraints on Unrelated Fields:** If a field that isn't always available for a new record is marked as `NOT NULL`.

## Examples of Insertion Anomalies

Let's illustrate with a couple of common scenarios:

---

### Example 1: New Employee without a Project

Imagine a single table called `Employee_Project` designed to store information about employees and the projects they are assigned to:

| EmployeeID | EmployeeName | EmployeeHireDate | ProjectID | ProjectName      | ProjectStartDate |
| :--------- | :----------- | :--------------- | :-------- | :--------------- | :--------------- |
| E101       | Alice Smith  | 2023-05-10       | P001      | Alpha System     | 2023-06-01       |
| E102       | Bob Johnson  | 2023-07-15       | P001      | Alpha System     | 2023-06-01       |
| E102       | Bob Johnson  | 2023-07-15       | P002      | Beta Enhancement | 2023-08-01       |

**The Anomaly:**

Suppose the company hires a new employee, Carol Davis, but she hasn't been assigned to any project yet.

* **Problem:** How do we add Carol to this table?
    * If `ProjectID` is part of the primary key or has a `NOT NULL` constraint, we cannot add Carol's information (EmployeeID, EmployeeName, EmployeeHireDate) without also providing a `ProjectID`.
    * We might be forced to insert a `NULL` value for `ProjectID`, `ProjectName`, and `ProjectStartDate` if the schema allows it. However, if the intention is that every row must describe a project assignment, then `ProjectID` might not allow NULLs.
    * Alternatively, we might have to create a "dummy" or "unassigned" project record just to be able to store Carol's employee details. This introduces inaccurate or placeholder data.

**Why it's an anomaly:** We are unable to insert a new employee's details (a valid piece of information) because information about their project assignment (another piece of information, which may not exist yet) is missing. The employee entity cannot be recorded independently of a project assignment.

---

### Example 2: New Course without Enrolled Students

Consider a table `Course_Enrollment` that stores details about courses and the students enrolled in them:

| CourseID | CourseName         | Instructor   | StudentID | StudentName | EnrollmentDate |
| :------- | :----------------- | :----------- | :-------- | :---------- | :------------- |
| CS101    | Intro to CS        | Dr. Turing   | S001      | John Doe    | 2023-09-01     |
| CS101    | Intro to CS        | Dr. Turing   | S002      | Jane Roe    | 2023-09-02     |
| MA202    | Calculus I         | Dr. Newton   | S003      | Peter Pan   | 2023-09-01     |

**The Anomaly:**

Suppose the university wants to add a new course, "Data Structures (CS205)" taught by Dr. Lovelace, to the catalog, but no students have enrolled in it yet.

* **Problem:** How do we add the new course details (CourseID, CourseName, Instructor) to this table?
    * If `StudentID` is part of the primary key or has a `NOT NULL` constraint, we cannot add the course information without also providing a `StudentID`.
    * We would be unable to list the new course until at least one student enrolls.

**Why it's an anomaly:** We cannot insert information about a new course (a valid entity) because information about an enrolled student (another entity, which doesn't exist yet for this course) is missing. The course entity cannot be recorded independently of student enrollment.

---

## How Normalization Helps

Database normalization is the process of organizing the columns and tables of a relational database to minimize data redundancy and improve data integrity. Properly normalizing a database helps eliminate insertion anomalies by:

1.  **Separating Entities:** Different types of information (like employees and projects, or courses and students) are stored in separate tables.
    * In Example 1, we would have an `Employees` table and a `Projects` table, and a linking table `Employee_Assignments` (with `EmployeeID` and `ProjectID`). This way, a new employee can be added to the `Employees` table without needing a project assignment.
    * In Example 2, we would have a `Courses` table and a `Students` table, and a linking table `Enrollments` (with `CourseID` and `StudentID`). A new course can be added to the `Courses` table without any student enrollments.
2.  **Ensuring Dependencies Make Sense:** Attributes in a table should depend only on the primary key of that table.

By adhering to normalization principles (like First Normal Form, Second Normal Form, Third Normal Form, etc.), designers can create database schemas that are less prone to such anomalies, leading to more robust, flexible, and maintainable databases.

## Conclusion

Insertion anomalies are a clear indicator that a database table is trying to store multiple, independent facts, leading to constraints that prevent the straightforward addition of valid data. Recognizing and resolving these anomalies through proper normalization is crucial for effective database design and management.

# The Six Classical Normal Forms in Database Normalization

Database normalization is a systematic process for organizing data in a relational database to minimize redundancy and eliminate undesirable characteristics like insertion, update, and deletion anomalies. This process involves applying a series of rules or tests, known as normal forms. While several normal forms have been defined, the six "classical" normal forms, building upon each other, are:

1.  **First Normal Form (1NF)**
2.  **Second Normal Form (2NF)**
3.  **Third Normal Form (3NF)**
4.  **Boyce-Codd Normal Form (BCNF)**
5.  **Fourth Normal Form (4NF)**
6.  **Fifth Normal Form (5NF)**

It's important to note that for a table to be in a higher normal form (e.g., 3NF), it must first satisfy all the requirements of the preceding normal forms (e.g., 1NF and 2NF).

Here's a breakdown of each:

---

## 1. First Normal Form (1NF)

**Objective:** Ensure atomicity of attributes and eliminate repeating groups.

**Requirements:**

* **Atomicity:** Each attribute (column) in a table must hold only atomic (indivisible) values. This means no multi-valued attributes or composite attributes within a single cell. For example, a single cell should not contain "John, Jane" or "123 Main St, Apt 4B". These should be broken down.
* **Unique Rows:** Each row in the table must be unique, typically enforced by a primary key.
* **No Repeating Groups:** There should be no repeating groups of columns. For instance, instead of having `Book1_Title`, `Book1_Author`, `Book2_Title`, `Book2_Author` in a single `Author_Books` table, books should be in a separate related table.
* **Same Data Type in Columns:** All values in a given column must be of the same data type.

**Anomalies Addressed:**
* Difficulties in querying and indexing multi-valued attributes.
* Wasted space and complexity with repeating groups.

---

## 2. Second Normal Form (2NF)

**Objective:** Eliminate partial dependencies on composite primary keys.

**Requirements:**

* The table must be in **1NF**.
* **No Partial Dependencies:** All non-key attributes (attributes not part of any candidate key) must be fully functionally dependent on the *entire* primary key. This rule applies specifically when the primary key is composite (made up of more than one attribute). A partial dependency occurs if a non-key attribute depends on only a part of the composite primary key.

**Anomalies Addressed:**
* **Update Anomaly:** If a fact related to part of the key changes, it might need to be updated in multiple rows, leading to inconsistencies if not all are updated.
* **Insertion Anomaly:** Cannot insert a fact about one part of the key without having information for the other part of the key.
* **Deletion Anomaly:** Deleting a record might unintentionally remove information about another entity that was only partially dependent on the key.

**Solution:** Decompose the table into smaller tables, separating the partially dependent attributes along with the part of the key they depend on.

---

## 3. Third Normal Form (3NF)

**Objective:** Eliminate transitive dependencies.

**Requirements:**

* The table must be in **2NF**.
* **No Transitive Dependencies:** No non-key attribute should be functionally dependent on another non-key attribute. In other words, non-key attributes must depend *directly* on the primary key, not indirectly through another non-key attribute.
    * A transitive dependency exists when A ‚Üí B and B ‚Üí C, where A is the primary key and B and C are non-key attributes. This implies A ‚Üí C transitively.

**Anomalies Addressed:**
* **Update Anomaly:** Information that is transitively dependent might be stored redundantly, requiring updates in multiple places.
* **Insertion Anomaly:** Cannot insert information about a non-key attribute if the other non-key attribute it depends on isn't present.
* **Deletion Anomaly:** Deleting a row might remove information about an entity that was transitively dependent.

**Solution:** Decompose the table to remove the transitive dependency, placing the transitively dependent attribute with the non-key attribute it depends on in a new table.

---

## 4. Boyce-Codd Normal Form (BCNF)

**Objective:** A stricter version of 3NF, ensuring every determinant is a candidate key.

**Requirements:**

* The table must be in **3NF**.
* **All Determinants are Candidate Keys:** For every non-trivial functional dependency X ‚Üí Y in the table, X must be a superkey (or a candidate key). A determinant is any attribute or set of attributes on which some other attribute is fully functionally dependent.

**Key Difference from 3NF:** 3NF allows non-key attributes to be determined by other non-key attributes if the determinant is not a candidate key *but* the dependent attribute is part of a candidate key. BCNF closes this loophole. Most tables in 3NF are also in BCNF. Cases where they differ are rare and usually involve tables with multiple overlapping candidate keys.

**Anomalies Addressed:**
* Addresses certain redundancies and anomalies that can still exist in 3NF tables with multiple overlapping candidate keys.

**Solution:** Decomposition, similar to previous normal forms, ensuring all determinants become candidate keys in the resulting tables.

---

## 5. Fourth Normal Form (4NF)

**Objective:** Eliminate multi-valued dependencies.

**Requirements:**

* The table must be in **BCNF**.
* **No Non-Trivial Multi-valued Dependencies (MVDs):** A table should not contain two or more independent multi-valued facts about an entity. An MVD exists when the presence of one row with a certain value for attribute A implies the presence of other rows with specific, independent sets of values for attributes B and C (i.e., A ‚Üí‚Üí B and A ‚Üí‚Üí C, where B and C are independent of each other).

**Anomalies Addressed:**
* Redundancy: To represent all combinations of the independent multi-valued attributes, many rows might be needed, leading to data duplication.
* Update, Insertion, and Deletion Anomalies related to managing these multiple independent facts in one table.

**Solution:** Decompose the table into two or more tables, each containing one of the multi-valued dependencies.

---

## 6. Fifth Normal Form (5NF) / Project-Join Normal Form (PJ/NF)

**Objective:** Eliminate join dependencies that are not implied by candidate keys.

**Requirements:**

* The table must be in **4NF**.
* **No Non-Trivial Join Dependencies that are not implied by Candidate Keys:** A table is in 5NF if every join dependency in it is implied by the candidate keys. A join dependency exists if a table can be reconstructed by joining multiple smaller tables (projections of the original table), and this reconstruction results in no spurious tuples (no extra, incorrect rows). If such a decomposition is possible without loss of information and is not based on candidate keys, the table is not in 5NF.

**Anomalies Addressed:**
* Addresses very complex redundancies that can occur when relationships between attributes are circular or cyclical and can only be losslessly decomposed into three or more tables.

**Practicality:** 5NF is rarely an issue in practical database design as such complex join dependencies are uncommon. Achieving 4NF usually handles most practical redundancy issues.

---

**In Summary:**

Most practical database designs aim for **3NF** or **BCNF** as these forms typically provide a good balance between data integrity, redundancy elimination, and performance. Higher normal forms (4NF, 5NF) address more complex and less common types of dependencies and redundancies.

# Decomposition Without Loss (Lossless-Join Decomposition)

In database normalization, **decomposition** is the process of breaking down a larger table (relation) into smaller, more manageable tables. The primary goals of decomposition are to reduce data redundancy, minimize anomalies (insertion, update, deletion), and improve data integrity.

However, it's crucial that this decomposition process is "without loss," meaning no information is lost or erroneously created when the smaller tables are joined back together. This property is formally known as **lossless-join decomposition** (or non-additive join decomposition).

## What is Lossless-Join Decomposition?

A decomposition of a relation $R$ into two or more sub-relations $R_1, R_2, \dots, R_n$ is considered a **lossless-join decomposition** if, when these sub-relations are joined back together using a natural join operation, the result is exactly the original relation $R$.

Mathematically:
If $R$ is decomposed into $R_1$ and $R_2$, the decomposition is lossless if:
$$R = R_1 \bowtie R_2$$
Where $\bowtie$ denotes the natural join operation.

If the natural join of the decomposed tables results in a table that contains more tuples (spurious tuples) or fewer tuples than the original table, then the decomposition is "lossy," and information has been lost or misrepresented.

## Why is Lossless Decomposition Important?

1.  **Data Integrity:** It ensures that the original data can be perfectly reconstructed from the decomposed tables. If a decomposition were lossy, queries involving data from multiple decomposed tables could produce incorrect or incomplete results.
2.  **Redundancy Reduction:** While the goal is to reduce redundancy, this must be done without losing the ability to accurately represent the original relationships between data attributes.
3.  **Anomaly Prevention:** Normalization aims to prevent anomalies. Lossless decomposition is a fundamental requirement for ensuring that the normalization process itself doesn't introduce new problems related to data retrieval.

## Conditions for Lossless-Join Decomposition

When a relation $R$ is decomposed into two sub-relations $R_1$ and $R_2$, the decomposition is guaranteed to be lossless if it satisfies the following three conditions with respect to a given set of functional dependencies (FDs) $F$:

1.  **Union of Attributes:** The union of the attributes in $R_1$ and $R_2$ must include all the attributes of the original relation $R$.
    $$Attributes(R_1) \cup Attributes(R_2) = Attributes(R)$$
    *(This ensures no attributes are dropped entirely).*

2.  **Non-Null Intersection:** The intersection of the attributes in $R_1$ and $R_2$ must not be empty. There must be at least one common attribute between $R_1$ and $R_2$.
    $$Attributes(R_1) \cap Attributes(R_2) \neq \emptyset$$
    *(This common attribute(s) is what allows the tables to be joined back together).*

3.  **Common Attribute as a Key:** The set of common attributes (the intersection) must be a superkey (or candidate key) for at least one of the sub-relations $R_1$ or $R_2$.
    Specifically, if $X = Attributes(R_1) \cap Attributes(R_2)$, then at least one of the following functional dependencies must be implied by the original set of FDs $F$ (i.e., be in $F^+$):
    * $X \rightarrow Attributes(R_1)$  (meaning X is a superkey for $R_1$)
    * **OR**
    * $X \rightarrow Attributes(R_2)$  (meaning X is a superkey for $R_2$)

If all three of these conditions hold, the decomposition of $R$ into $R_1$ and $R_2$ is lossless. When decomposing into more than two relations, the principle is applied iteratively: you can join two relations losslessly to form a new intermediate relation, and then join that with another relation losslessly, and so on.

## Example of Lossless Decomposition

Consider a relation `Employee_Department` with attributes `(EmpID, EmpName, EmpStreet, EmpCity, DeptID, DeptName, DeptManagerID)` and the following functional dependencies:
* `EmpID` $\rightarrow$ `EmpName, EmpStreet, EmpCity, DeptID`
* `DeptID` $\rightarrow$ `DeptName, DeptManagerID`

This table is not in 3NF because `DeptName` and `DeptManagerID` are transitively dependent on `EmpID` (via `DeptID`).

Let's decompose it into two relations:
* `R1 (EmpID, EmpName, EmpStreet, EmpCity, DeptID)`
* `R2 (DeptID, DeptName, DeptManagerID)`

Let's check the conditions for lossless join:

1.  **Union of Attributes:**
    `{EmpID, EmpName, EmpStreet, EmpCity, DeptID}` $\cup$ `{DeptID, DeptName, DeptManagerID}` $=$ `{EmpID, EmpName, EmpStreet, EmpCity, DeptID, DeptName, DeptManagerID}`.
    This is equal to the attributes of the original relation. (Condition met)

2.  **Non-Null Intersection:**
    `{EmpID, EmpName, EmpStreet, EmpCity, DeptID}` $\cap$ `{DeptID, DeptName, DeptManagerID}` $=$ `{DeptID}`.
    The intersection is not empty. (Condition met)

3.  **Common Attribute as a Key:**
    The common attribute is `DeptID`.
    * Is `DeptID` a superkey for `R1`? No, `DeptID` does not determine `EmpID`, `EmpName`, etc.
    * Is `DeptID` a superkey for `R2`? Yes, because `DeptID` $\rightarrow$ `DeptName, DeptManagerID` is given. Thus, `DeptID` $\rightarrow$ `Attributes(R2)`. (Condition met)

Since all three conditions are satisfied, this decomposition is **lossless**. We can join `R1` and `R2` on `DeptID` to reconstruct the original `Employee_Department` relation without any loss of information or creation of spurious tuples.

## Lossy Decomposition (What to Avoid)

If the above conditions are not met, particularly the third one, the decomposition might be "lossy."

**Example of a Potentially Lossy Decomposition:**

Consider `R(A, B, C)` with FD `A` $\rightarrow$ `B`.
If we decompose `R` into `R1(A, C)` and `R2(B, C)`.

1.  **Union:** `{A, C}` $\cup$ `{B, C}` $=$ `{A, B, C}`. (Met)
2.  **Intersection:** `{A, C}` $\cap$ `{B, C}` $=$ `{C}`. (Met)
3.  **Common Attribute as Key:** The common attribute is `C`.
    * Is `C` $\rightarrow$ `{A, C}`? Not implied by `A` $\rightarrow$ `B`.
    * Is `C` $\rightarrow$ `{B, C}`? Not implied by `A` $\rightarrow$ `B`.

Since condition 3 is not met, this decomposition is likely lossy. If you join `R1` and `R2` on `C`, you might generate tuples that were not in the original relation `R`, leading to incorrect data. For example:

Original `R`:

| A  | B  | C  |
|----|----|----|
| a1 | b1 | c1 |
| a2 | b2 | c1 |

`R1(A, C)`:

| A  | C  |
|----|----|
| a1 | c1 |
| a2 | c1 |

`R2(B, C)`:

| B  | C  |
|----|----|
| b1 | c1 |
| b2 | c1 |

Natural join of `R1` and `R2` on `C`:

| A  | B  | C  |
|----|----|----|
| a1 | b1 | c1 |
| a1 | b2 | c1 |  <-- Spurious tuple
| a2 | b1 | c1 |  <-- Spurious tuple
| a2 | b2 | c1 |

The tuples `(a1, b2, c1)` and `(a2, b1, c1)` were not in the original relation. This demonstrates a lossy decomposition where information is incorrectly generated.

## Conclusion

Lossless-join decomposition is a fundamental property that must be ensured during database normalization. It guarantees that breaking down tables to reduce redundancy and anomalies does not compromise the ability to accurately retrieve and reconstruct the original information. The conditions for lossless join provide a formal way to verify this property.

# 3NF Synthesis Algorithm (Lossless Join and Dependency Preserving)

This algorithm takes a relation schema $R$ and a set of functional dependencies $F$ defined on $R$ and produces a decomposition of $R$ into a set of relation schemas $\{R_1, R_2, ..., R_n\}$ such that each $R_i$ is in 3NF, the decomposition is lossless join, and the decomposition is dependency preserving.

**Input:**

* A relation schema $R = \{A_1, A_2, ..., A_m\}$ (a set of attributes).
* A set of functional dependencies $F$ on $R$.

**Output:**

* A decomposition of $R$ into a set of relation schemas $\{R_1, R_2, ..., R_n\}$ satisfying the 3NF, lossless join, and dependency preservation properties.

**Algorithm Steps:**

1.  **Find a Minimal Cover of F:**
    * Compute a minimal cover $F_c$ for the set of functional dependencies $F$. A minimal cover is a set of functional dependencies that is equivalent to $F$ (i.e., $F_c^+ = F^+$) and satisfies the following conditions:
        * Every functional dependency in $F_c$ has a single attribute on the right-hand side.
        * There are no extraneous attributes on the left-hand side of any functional dependency in $F_c$. (An attribute $A$ is extraneous in $X \to Y$ if $F \models (X - \{A\}) \to Y$).
        * There are no redundant functional dependencies in $F_c$. (A functional dependency $f \in F_c$ is redundant if $F_c - \{f\} \models f$).

    * *Note: Finding a minimal cover can be a multi-step process involving making right-hand sides singletons, removing extraneous attributes from left-hand sides, and removing redundant dependencies.*

2.  **Create Relation Schemas from Minimal Cover:**
    * For each functional dependency $X \to A$ in the minimal cover $F_c$, create a new relation schema $R_i$ with the attributes $\{X \cup \{A\}\}$.
    * If there are multiple functional dependencies with the same left-hand side $X$, say $X \to A_1, X \to A_2, ..., X \to A_k$, you can group them and create a single relation schema with attributes $\{X \cup \{A_1, A_2, ..., A_k\}\}$. This often leads to fewer relations in the decomposition.

3.  **Handle Attributes Not Included:**
    * Check if all attributes in the original schema $R$ are included in at least one of the created relation schemas $\{R_1, R_2, ..., R_k\}$.
    * If there are any attributes in $R$ that are not part of any $R_i$ created in step 2, these attributes must be part of a candidate key of $R$. This situation is less common with a complete minimal cover derived from all FDs. However, if it occurs, proceed to step 4.

4.  **Ensure Lossless Join Property (Add a Key Relation if Necessary):**
    * If none of the relation schemas created in step 2 (and potentially step 3 if needed) contain a candidate key of the original relation $R$, create an additional relation schema $R_{key}$ consisting of the attributes of *any* one candidate key of $R$.
    * This step is crucial for guaranteeing the lossless join property. A decomposition is lossless if and only if for every pair of relations $R_i, R_j$ in the decomposition, their intersection of attributes forms a superkey for at least one of the relations $R_i$ or $R_j$, OR if a relation in the decomposition contains a key of the original relation. Including a relation with a candidate key of $R$ guarantees the latter condition indirectly for the overall decomposition process when using this synthesis approach.

5.  **Eliminate Redundant Relations:**
    * Examine the set of relation schemas created. If any relation schema $R_i$ is a subset of another relation schema $R_j$ (i.e., all attributes in $R_i$ are also in $R_j$), then $R_i$ can be eliminated.

**Why this algorithm guarantees the properties:**

* **3NF:** By creating relations directly from the functional dependencies in a minimal cover, the algorithm ensures that the resulting relations are in 3NF. Functional dependencies within each new relation are based on the left-hand side determining the right-hand side. Since the minimal cover has single attributes on the right-hand side and no extraneous attributes on the left, and because we are grouping FDs with the same determinant, we avoid partial and transitive dependencies within the constructed relations relative to their derived keys.
* **Dependency Preservation:** Since every functional dependency in the minimal cover $F_c$ (which is equivalent to the original set $F$) is directly represented in one of the created relation schemas, all original dependencies can be checked by examining only the individual relations in the decomposition without the need for joins.
* **Lossless Join:** Step 4 is key to ensuring the lossless join property. While the construction from the minimal cover often implicitly provides this, explicitly adding a relation containing a candidate key of $R$ guarantees that the original relation can be reconstructed by naturally joining the decomposed relations.

This algorithm provides a systematic way to decompose a relation into 3NF while formally preserving dependencies and ensuring a lossless join, which are essential for maintaining data integrity and avoiding spurious tuples.

# Fourth Normal Form (4NF) in Database Normalization

## Overview

Fourth Normal Form (4NF) is a level of database normalization that addresses **multi-valued dependencies**. It builds upon the requirements of **Boyce-Codd Normal Form (BCNF)** and ensures that a relation does not contain **two or more independent multi-valued facts** about an entity.

## Prerequisites

A relation must be in **Boyce-Codd Normal Form (BCNF)** before it can be considered for 4NF.

## What is a Multi-Valued Dependency?

A **multi-valued dependency (MVD)** occurs when, for a single attribute in a relation, there are multiple independent values of another attribute, not related to each other.

### Example:

Suppose we have a relation that stores the following information:

- A professor can teach multiple subjects.
- A professor can speak multiple languages.

These two attributes (subjects and languages) are independent multi-valued facts about a professor.

| Professor | Subject  | Language |
|-----------|----------|----------|
| Smith     | Math     | English  |
| Smith     | Math     | French   |
| Smith     | Physics  | English  |
| Smith     | Physics  | French   |

This leads to **unnecessary repetition** and can introduce **anomalies**.

## 4NF Rule

A relation is in **Fourth Normal Form (4NF)** if **it is in BCNF** and **it has no multi-valued dependencies**, except for those where the multi-valued dependency is a consequence of a candidate key.

## How to Normalize to 4NF

To remove multi-valued dependencies:

1. **Identify** independent multi-valued facts.
2. **Decompose** the relation into separate relations for each multi-valued fact.

### Example (continued):

Decompose the above relation into two:

**Prof_Subject**

| Professor | Subject  |
|-----------|----------|
| Smith     | Math     |
| Smith     | Physics  |

**Prof_Language**

| Professor | Language |
|-----------|----------|
| Smith     | English  |
| Smith     | French   |

Now, each table contains only one multi-valued fact and avoids unnecessary redundancy.

## Summary

- 4NF deals with **multi-valued dependencies**.
- It requires the relation to be in **BCNF**.
- It removes **independent multi-valued facts** into separate relations.
- It ensures better data integrity and reduces redundancy.

